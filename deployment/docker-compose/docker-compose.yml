services:
  core_backend:
    image: public.ecr.aws/j3r7b4k0/momconnect-aaq/core_backend:latest
    build:
      context: ../../core_backend
      dockerfile: Dockerfile
    command: >
      /bin/sh startup.sh
    restart: always
    volumes:
      - temp:/usr/src/aaq_backend/temp
      - ./.gcp_credentials.json:/app/credentials.json
    env_file:
      - .base.env
      - .core_backend.env
      - .litellm_proxy.env
    environment:
      - REDIS_HOST=redis://redis:6379
      - LITELLM_ENDPOINT=http://huggingface-embeddings
    depends_on:
      - redis

    develop:
      watch:
        - action: rebuild
          path: ../../core_backend

  admin_app:
    image: public.ecr.aws/j3r7b4k0/momconnect-aaq/admin_app:latest
    build:
      context: ../../admin_app
      dockerfile: Dockerfile
    command: >
      node server.js
    depends_on:
      - core_backend
    restart: always
    env_file:
      - .base.env
    develop:
      watch:
        - action: rebuild
          path: ../../admin_app

  caddy:
    image: caddy:2.7.6
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    env_file:
      - .base.env

  huggingface-embeddings:
    # image either refers to locally built image or defaults to the one from the registry
    image: ${EMBEDDINGS_IMAGE_NAME:-ghcr.io/huggingface/text-embeddings-inference:cpu-1.5}
    profiles:
      - huggingface-embeddings
      - optional-components
    volumes:
      - $PWD/data:/data
    command:
      ["--model-id", "/data/gte-large-en-v1.5", "--api-key", "${LITELLM_API_KEY}"]
    restart: always
    env_file:
      - .litellm_proxy.env

  redis:
    image: "redis:6.0-alpine"
    ports:
      # Expose the port to port 6380 on the host machine for debugging
      - "6380:6379"
    restart: always

volumes:
  caddy_data:
  caddy_config:
  temp:
